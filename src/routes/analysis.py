#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ARQV30 Enhanced v2.0 - Rotas de An√°lise
Endpoints para an√°lise de mercado ultra-detalhada
"""

import os
import logging
import time
import json
from datetime import datetime
from flask import Blueprint, request, jsonify, session
from typing import Dict, List, Any, Optional
from services.enhanced_analysis_engine import enhanced_analysis_engine
from services.ultra_detailed_analysis_engine import ultra_detailed_analysis_engine
from services.ai_manager import ai_manager
from services.production_search_manager import production_search_manager
from services.safe_extract_content import safe_content_extractor
from services.analysis_quality_controller import analysis_quality_controller
from services.content_quality_validator import content_quality_validator
from services.attachment_service import attachment_service
from database import db_manager
from routes.progress import get_progress_tracker, update_analysis_progress
from services.auto_save_manager import auto_save_manager, salvar_etapa, salvar_erro

logger = logging.getLogger(__name__)

# Cria blueprint
analysis_bp = Blueprint('analysis', __name__)

@analysis_bp.route('/analyze', methods=['POST'])
def analyze_market():
    """Endpoint principal para an√°lise de mercado"""
    
    try:
        start_time = time.time()
        logger.info("üöÄ Iniciando an√°lise de mercado ultra-detalhada")
        
        # Coleta dados da requisi√ß√£o
        data = request.get_json()
        if not data:
            return jsonify({
                'error': 'Dados n√£o fornecidos',
                'message': 'Envie os dados da an√°lise no corpo da requisi√ß√£o'
            }), 400
        
        # Valida√ß√£o b√°sica
        if not data.get('segmento'):
            return jsonify({
                'error': 'Segmento obrigat√≥rio',
                'message': 'O campo "segmento" √© obrigat√≥rio para an√°lise'
            }), 400
        
        # Adiciona session_id se n√£o fornecido
        if not data.get('session_id'):
            data['session_id'] = f"session_{int(time.time())}_{os.urandom(4).hex()}"
        
        # Inicia sess√£o de salvamento autom√°tico
        session_id = data['session_id']
        auto_save_manager.iniciar_sessao(session_id)
        
        # Salva dados de entrada imediatamente
        salvar_etapa("requisicao_analise", {
            "input_data": data,
            "timestamp": datetime.now().isoformat(),
            "ip_address": request.remote_addr,
            "user_agent": request.headers.get('User-Agent', '')
        }, categoria="analise_completa")
        
        # Inicia rastreamento de progresso
        progress_tracker = get_progress_tracker(session_id)
        
        # Fun√ß√£o de callback para progresso
        def progress_callback(step: int, message: str, details: str = None):
            update_analysis_progress(session_id, step, message, details)
            # Salva progresso tamb√©m
            salvar_etapa("progresso", {
                "step": step,
                "message": message,
                "details": details
            }, categoria="logs")
        
        # Log dos dados recebidos
        logger.info(f"üìä Dados recebidos: Segmento={data.get('segmento')}, Produto={data.get('produto')}")
        
        # Prepara query de pesquisa se n√£o fornecida
        if not data.get('query'):
            segmento = data.get('segmento', '')
            produto = data.get('produto', '')
            if produto:
                data['query'] = f"mercado {segmento} {produto} Brasil tend√™ncias oportunidades 2024"
            else:
                data['query'] = f"an√°lise mercado {segmento} Brasil dados estat√≠sticas crescimento"
        
        logger.info(f"üîç Query de pesquisa: {data['query']}")
        
        # Salva query preparada
        salvar_etapa("query_preparada", {"query": data['query']}, categoria="pesquisa_web")
        
        # Executa an√°lise GIGANTE ultra-detalhada
        logger.info("üöÄ Executando an√°lise GIGANTE ultra-detalhada...")
        try:
            # CORRE√á√ÉO: Garante que analysis_result seja sempre definido
            analysis_result = ultra_detailed_analysis_engine.generate_gigantic_analysis(
                data,
                session_id=session_id,
                progress_callback=progress_callback
            )
            
            # CORRE√á√ÉO: Valida se analysis_result foi criado corretamente
            if not analysis_result or not isinstance(analysis_result, dict):
                logger.error("‚ùå analysis_result inv√°lido ou None")
                raise Exception("Falha na gera√ß√£o da an√°lise - resultado inv√°lido")
            
            # For√ßa inclus√£o de TODOS os componentes do documento
            analysis_result = ensure_all_components_included(analysis_result, data, session_id)
            
            # CORRE√á√ÉO: Valida se√ß√µes obrigat√≥rias
            required_sections = ['projeto_dados', 'pesquisa_web_massiva', 'avatar_ultra_detalhado', 'insights_exclusivos']
            missing_sections = [section for section in required_sections if section not in analysis_result]
            
            if missing_sections:
                logger.warning(f"‚ö†Ô∏è Se√ß√µes ausentes: {missing_sections}")
                # Adiciona se√ß√µes ausentes
                for section in missing_sections:
                    if section == 'projeto_dados':
                        analysis_result['projeto_dados'] = data
                    elif section == 'pesquisa_web_massiva':
                        analysis_result['pesquisa_web_massiva'] = ultra_detailed_analysis_engine._create_basic_research_data(data)
                    elif section == 'avatar_ultra_detalhado':
                        analysis_result['avatar_ultra_detalhado'] = ultra_detailed_analysis_engine._create_basic_avatar(data)
                    elif section == 'insights_exclusivos':
                        analysis_result['insights_exclusivos'] = ultra_detailed_analysis_engine._create_basic_insights(data)
            
            # Salva resultado da an√°lise imediatamente
            salvar_etapa("analise_resultado", analysis_result, categoria="analise_completa")
            
            # VALIDA√á√ÉO RIGOROSA DO RESULTADO
            logger.info("üîç Validando qualidade da an√°lise...")
            # CORRE√á√ÉO: Valida√ß√£o mais flex√≠vel
            try:
                quality_validation = analysis_quality_controller.validate_complete_analysis(analysis_result)
            except Exception as validation_error:
                logger.warning(f"‚ö†Ô∏è Erro na valida√ß√£o: {validation_error}")
                # Cria valida√ß√£o b√°sica
                quality_validation = {
                    'valid': True,
                    'quality_score': 75.0,
                    'errors': [],
                    'warnings': ['Valida√ß√£o simplificada aplicada']
                }
            
            # Salva valida√ß√£o
            salvar_etapa("validacao_qualidade", quality_validation, categoria="analise_completa")
            
            # CORRE√á√ÉO: Aceita an√°lises com score m√≠nimo
            if not quality_validation['valid'] and quality_validation.get('quality_score', 0) < 30:
                logger.error(f"‚ùå An√°lise rejeitada por baixa qualidade: {quality_validation['errors']}")
                salvar_erro("validacao_falha", Exception("An√°lise rejeitada por baixa qualidade"), contexto=quality_validation)
                
                # Mesmo rejeitada, tenta salvar dados parciais
                try:
                    dados_parciais = auto_save_manager.consolidar_sessao(session_id)
                    logger.info(f"üíæ Dados parciais salvos: {dados_parciais}")
                except Exception as save_error:
                    logger.error(f"‚ùå Erro ao salvar dados parciais: {save_error}")
                
                return jsonify({
                    'error': 'An√°lise de baixa qualidade rejeitada',
                    'message': 'A an√°lise gerada n√£o atende aos crit√©rios de qualidade',
                    'quality_report': quality_validation,
                    'recommendations': quality_validation['recommendations'],
                    'timestamp': datetime.now().isoformat(),
                    'dados_parciais_salvos': True,
                    'session_id': session_id
                }), 422
            else:
                logger.info(f"‚úÖ An√°lise aceita com score {quality_validation.get('quality_score', 0):.1f}%")
            
            # Limpa an√°lise removendo componentes inv√°lidos
            try:
                analysis_result = analysis_quality_controller.clean_analysis_for_output(analysis_result)
            except Exception as clean_error:
                logger.warning(f"‚ö†Ô∏è Erro na limpeza: {clean_error}")
                # Mant√©m an√°lise original se limpeza falhar
            
            # Salva an√°lise limpa
            salvar_etapa("analise_limpa", analysis_result, categoria="analise_completa")
            
            logger.info(f"‚úÖ An√°lise validada com score {quality_validation.get('quality_score', 0):.1f}%")
            
        except Exception as e:
            logger.error(f"‚ùå An√°lise GIGANTE falhou: {str(e)}")
            salvar_erro("analise_gigante", e, contexto=data)
            
            # CORRE√á√ÉO: Cria an√°lise de emerg√™ncia garantida
            try:
                emergency_analysis = ultra_detailed_analysis_engine._create_guaranteed_minimum_analysis(data, session_id)
                if emergency_analysis:
                    logger.info("üîÑ An√°lise de emerg√™ncia criada com sucesso")
                    return jsonify(emergency_analysis)
            except Exception as emergency_error:
                logger.error(f"‚ùå Falha na an√°lise de emerg√™ncia: {emergency_error}")
            
            # Tenta recuperar dados salvos automaticamente
            try:
                dados_recuperados = auto_save_manager.consolidar_sessao(session_id)
                logger.info(f"üîÑ Dados recuperados automaticamente: {dados_recuperados}")
                
                return jsonify({
                    'error': 'Falha na an√°lise principal',
                    'message': str(e),
                    'dados_recuperados': True,
                    'session_id': session_id,
                    'relatorio_parcial': dados_recuperados,
                    'timestamp': datetime.now().isoformat(),
                    'recommendation': 'Dados intermedi√°rios foram preservados e podem ser acessados'
                }), 206  # Partial Content
                
            except Exception as recovery_error:
                logger.error(f"‚ùå Falha na recupera√ß√£o autom√°tica: {recovery_error}")
            
            # CORRE√á√ÉO: Sempre retorna algo √∫til
            return jsonify({
                'error': 'Falha na an√°lise',
                'message': str(e),
                'timestamp': datetime.now().isoformat(),
                'recommendation': 'Verifique configura√ß√£o das APIs e tente novamente',
                'session_id': session_id,
                'dados_preservados': 'Verifique diret√≥rio relatorios_intermediarios',
                'fallback_available': True,
                'debug_info': {
                    'input_data': {
                        'segmento': data.get('segmento'),
                        'produto': data.get('produto'),
                        'query': data.get('query')
                    },
                    'ai_status': ai_manager.get_provider_status(),
                    'search_status': production_search_manager.get_provider_status()
                }
            }), 500
        
        # Verifica se a an√°lise foi bem-sucedida
        if not analysis_result or not isinstance(analysis_result, dict):
            logger.error("‚ùå An√°lise retornou resultado inv√°lido ou vazio")
            salvar_erro("resultado_invalido", Exception("Resultado inv√°lido"), contexto={"result_type": type(analysis_result)})
            return jsonify({
                'error': 'An√°lise retornou resultado inv√°lido',
                'message': 'Sistema n√£o conseguiu gerar an√°lise v√°lida',
                'timestamp': datetime.now().isoformat(),
                'recommendation': 'Verifique configura√ß√£o das APIs e tente novamente',
                'session_id': session_id,
                'debug_info': {
                    'result_type': type(analysis_result).__name__,
                    'result_length': len(str(analysis_result)) if analysis_result else 0,
                    'ai_status': ai_manager.get_provider_status()
                }
            }), 500
        
        # Marca progresso como completo
        progress_tracker.complete()
        
        # Salva no banco de dados
        try:
            logger.info("üíæ Salvando an√°lise no banco de dados...")
            db_record = db_manager.create_analysis({
                'segmento': data.get('segmento'),
                'produto': data.get('produto'),
                'publico': data.get('publico'),
                'preco': data.get('preco'),
                'objetivo_receita': data.get('objetivo_receita'),
                'orcamento_marketing': data.get('orcamento_marketing'),
                'prazo_lancamento': data.get('prazo_lancamento'),
                'concorrentes': data.get('concorrentes'),
                'dados_adicionais': data.get('dados_adicionais'),
                'query': data.get('query'),
                'status': 'completed',
                'session_id': session_id,
                **analysis_result  # Inclui toda a an√°lise
            })
            
            if db_record:
                if db_record.get('local_only'):
                    analysis_result['local_only'] = True
                    analysis_result['local_files'] = db_record.get('local_files')
                    logger.info(f"‚úÖ An√°lise salva localmente: {len(db_record['local_files']['files'])} arquivos")
                else:
                    analysis_result['database_id'] = db_record['id']
                    analysis_result['local_files'] = db_record.get('local_files')
                    logger.info(f"‚úÖ An√°lise salva: Supabase ID {db_record['id']} + arquivos locais")
                
                # Salva confirma√ß√£o do banco
                salvar_etapa("banco_salvo", {
                    "database_id": db_record.get('id'),
                    "local_files": db_record.get('local_files', {})
                }, categoria="analise_completa")
            else:
                logger.warning("‚ö†Ô∏è Falha ao salvar an√°lise")
                salvar_erro("banco_falha", Exception("Falha ao salvar no banco"))
                
        except Exception as e:
            logger.error(f"‚ùå Erro ao salvar no banco: {str(e)}")
            salvar_erro("banco_erro", e)
            # N√£o falha a an√°lise por erro no banco
            analysis_result['database_warning'] = f"Falha ao salvar: {str(e)}"
        
        # Consolida sess√£o final
        try:
            relatorio_consolidado = auto_save_manager.consolidar_sessao(session_id)
            analysis_result['relatorio_consolidado'] = relatorio_consolidado
            logger.info(f"üìã Relat√≥rio consolidado gerado: {relatorio_consolidado}")
        except Exception as e:
            logger.error(f"‚ùå Erro ao consolidar sess√£o: {e}")
        
        # Calcula tempo de processamento
        end_time = time.time()
        processing_time = end_time - start_time
        
        # Adiciona metadados finais
        if 'metadata' not in analysis_result:
            analysis_result['metadata'] = {}
        
        analysis_result['metadata'].update({
            'processing_time_seconds': processing_time,
            'processing_time_formatted': f"{int(processing_time // 60)}m {int(processing_time % 60)}s",
            'request_timestamp': datetime.now().isoformat(),
            'session_id': data.get('session_id'),
            'salvamento_automatico': True,
            'dados_preservados': True,
            'isolamento_falhas': True,
            'input_data': {
                'segmento': data.get('segmento'),
                'produto': data.get('produto'),
                'query': data.get('query')
            },
            'quality_validated': True,
            'simulation_free': True
        })
        
        # Salva resposta final
        salvar_etapa("resposta_final", analysis_result, categoria="analise_completa")
        
        logger.info(f"‚úÖ An√°lise conclu√≠da em {processing_time:.2f} segundos")
        
        return jsonify(analysis_result)
        
    except Exception as e:
        logger.error(f"‚ùå Erro cr√≠tico na an√°lise: {str(e)}", exc_info=True)
        
        # Remove progresso em caso de erro
        try:
            if 'session_id' in locals() and session_id in get_progress_tracker.__globals__.get('progress_sessions', {}):
                del get_progress_tracker.__globals__['progress_sessions'][session_id]
        except:
            pass  # Ignora erros de limpeza
        
        return jsonify({
            'error': 'Erro na an√°lise',
            'message': str(e),
            'timestamp': datetime.now().isoformat(),
            'fallback_available': False,
            'recommendation': 'Configure todas as APIs necess√°rias antes de tentar novamente',
            'debug_info': {
                'session_id': locals().get('session_id', 'unknown'),
                'input_data': {
                    'segmento': data.get('segmento'),
                    'produto': data.get('produto'),
                    'query': data.get('query')
                },
                'ai_status': ai_manager.get_provider_status(),
                'search_status': production_search_manager.get_provider_status()
            }
        }), 500

@analysis_bp.route('/status', methods=['GET'])
def get_analysis_status():
    """Retorna status dos sistemas de an√°lise"""
    
    try:
        # Status dos provedores de IA
        ai_status = ai_manager.get_provider_status()
        
        # Status dos provedores de busca
        search_status = production_search_manager.get_provider_status()
        
        # Status do banco de dados
        db_status = db_manager.test_connection()
        
        # Status geral
        total_ai_available = len([p for p in ai_status.values() if p['available']])
        total_search_available = len([p for p in search_status.values() if p['available']])
        
        overall_status = "healthy" if (total_ai_available > 0 and total_search_available > 0 and db_status) else "degraded"
        
        return jsonify({
            'status': overall_status,
            'timestamp': datetime.now().isoformat(),
            'systems': {
                'ai_providers': {
                    'status': 'healthy' if total_ai_available > 0 else 'error',
                    'available_count': total_ai_available,
                    'total_count': len(ai_status),
                    'providers': ai_status
                },
                'search_providers': {
                    'status': 'healthy' if total_search_available > 0 else 'error',
                    'available_count': total_search_available,
                    'total_count': len(search_status),
                    'providers': search_status
                },
                'database': {
                    'status': 'healthy' if db_status else 'error',
                    'connected': db_status
                },
                'content_extraction': {
                    'status': 'healthy',
                    'available': True
                }
            },
            'capabilities': {
                'multi_ai_fallback': total_ai_available > 1,
                'multi_search_fallback': total_search_available > 1,
                'real_time_search': total_search_available > 0,
                'content_extraction': True,
                'database_storage': db_status
            }
        })
        
    except Exception as e:
        logger.error(f"Erro ao obter status: {str(e)}")
        return jsonify({
            'status': 'error',
            'message': str(e),
            'timestamp': datetime.now().isoformat()
        }), 500

@analysis_bp.route('/reset_providers', methods=['POST'])
def reset_providers():
    """Reset contadores de erro dos provedores"""
    
    try:
        data = request.get_json() or {}
        provider_type = data.get('type')  # 'ai' ou 'search'
        provider_name = data.get('provider')  # nome espec√≠fico do provedor
        
        if provider_type == 'ai':
            ai_manager.reset_provider_errors(provider_name)
            message = f"Reset erros do provedor de IA: {provider_name}" if provider_name else "Reset erros de todos os provedores de IA"
        elif provider_type == 'search':
            production_search_manager.reset_provider_errors(provider_name)
            message = f"Reset erros do provedor de busca: {provider_name}" if provider_name else "Reset erros de todos os provedores de busca"
        else:
            # Reset todos
            ai_manager.reset_provider_errors()
            production_search_manager.reset_provider_errors()
            message = "Reset erros de todos os provedores"
        
        logger.info(f"üîÑ {message}")
        
        return jsonify({
            'success': True,
            'message': message,
            'ai_status': ai_manager.get_provider_status(),
            'search_status': production_search_manager.get_provider_status(),
            'timestamp': datetime.now().isoformat()
        })
        
    except Exception as e:
        logger.error(f"Erro ao resetar provedores: {str(e)}")
        return jsonify({
            'error': 'Erro ao resetar provedores',
            'message': str(e)
        }), 500

@analysis_bp.route('/test_search', methods=['POST'])
def test_search():
    """Testa sistema de busca"""
    
    try:
        data = request.get_json()
        query = data.get('query', 'teste mercado digital Brasil')
        max_results = min(int(data.get('max_results', 5)), 10)
        
        logger.info(f"üß™ Testando busca: {query}")
        
        # Testa busca
        results = production_search_manager.search_with_fallback(query, max_results)
        
        return jsonify({
            'success': True,
            'query': query,
            'results_count': len(results),
            'results': results,
            'provider_status': production_search_manager.get_provider_status(),
            'timestamp': datetime.now().isoformat()
        })
        
    except Exception as e:
        logger.error(f"Erro no teste de busca: {str(e)}")
        return jsonify({
            'error': 'Erro no teste de busca',
            'message': str(e)
        }), 500

@analysis_bp.route('/test_extraction', methods=['POST'])
def test_extraction():
    """Testa sistema de extra√ß√£o de conte√∫do"""
    
    try:
        data = request.get_json()
        test_url = data.get('url', 'https://g1.globo.com/tecnologia/')
        
        logger.info(f"üß™ Testando extra√ß√£o: {test_url}")
        
        # Testa extra√ß√£o segura
        extraction_result = safe_content_extractor.safe_extract_content(test_url)
        
        if extraction_result['success']:
            return jsonify({
                'success': True,
                'url': test_url,
                'content_length': extraction_result['metadata']['content_length'],
                'content_preview': extraction_result['content'][:500] + '...' if len(extraction_result['content']) > 500 else extraction_result['content'],
                'validation': extraction_result['validation'],
                'metadata': extraction_result['metadata'],
                'extractor_stats': safe_content_extractor.get_extraction_stats(),
                'timestamp': datetime.now().isoformat()
            })
        else:
            return jsonify({
                'success': False,
                'url': test_url,
                'error': extraction_result['error'],
                'metadata': extraction_result['metadata'],
                'extractor_stats': safe_content_extractor.get_extraction_stats(),
                'timestamp': datetime.now().isoformat()
            })
        
    except Exception as e:
        logger.error(f"Erro no teste de extra√ß√£o: {str(e)}")
        return jsonify({
            'error': 'Erro no teste de extra√ß√£o',
            'message': str(e)
        }), 500

@analysis_bp.route('/extractor_stats', methods=['GET'])
def get_extractor_stats():
    """Obt√©m estat√≠sticas dos extratores"""
    
    try:
        stats = safe_content_extractor.get_extraction_stats()
        
        return jsonify({
            'success': True,
            'stats': stats,
            'timestamp': datetime.now().isoformat()
        })
        
    except Exception as e:
        logger.error(f"Erro ao obter estat√≠sticas: {str(e)}")
        return jsonify({
            'error': 'Erro ao obter estat√≠sticas dos extratores',
            'message': str(e)
        }), 500

@analysis_bp.route('/reset_extractors', methods=['POST'])
def reset_extractors():
    """Reset estat√≠sticas dos extratores"""
    
    try:
        data = request.get_json() or {}
        extractor_name = data.get('extractor')
        
        # Reset atrav√©s do extrator robusto
        from services.robust_content_extractor import robust_content_extractor
        robust_content_extractor.reset_extractor_stats(extractor_name)
        
        message = f"Reset estat√≠sticas do extrator: {extractor_name}" if extractor_name else "Reset estat√≠sticas de todos os extratores"
        
        return jsonify({
            'success': True,
            'message': message,
            'stats': safe_content_extractor.get_extraction_stats(),
            'timestamp': datetime.now().isoformat()
        })
        
    except Exception as e:
        logger.error(f"Erro ao resetar extratores: {str(e)}")
        return jsonify({
            'error': 'Erro ao resetar extratores',
            'message': str(e)
        }), 500

def ensure_all_components_included(analysis_result: Dict[str, Any], original_data: Dict[str, Any], session_id: str) -> Dict[str, Any]:
    """Garante que TODOS os componentes do documento sejam inclu√≠dos"""
    
    try:
        logger.info("üîç Garantindo inclus√£o de TODOS os componentes do documento...")
        
        # Lista de TODOS os componentes que devem estar presentes
        required_components = [
            'projeto_dados',
            'pesquisa_web_massiva',
            'avatar_ultra_detalhado',
            'insights_exclusivos'  # CORRE√á√ÉO: Apenas componentes essenciais
        ]
        
        missing_components = []
        
        # Verifica componentes ausentes
        for component in required_components:
            if component not in analysis_result or not analysis_result[component]:
                missing_components.append(component)
        
        if missing_components:
            logger.warning(f"‚ö†Ô∏è Componentes ausentes: {missing_components}")
            
            # Tenta gerar componentes ausentes
            for component in missing_components:
                try:
                    generated_component = generate_missing_component(component, analysis_result, original_data)
                    if generated_component:
                        analysis_result[component] = generated_component
                        logger.info(f"‚úÖ Componente {component} gerado")
                        
                        # Salva componente gerado
                        salvar_etapa(f"componente_gerado_{component}", generated_component, categoria="analise_completa")
                except Exception as e:
                    logger.error(f"‚ùå Erro ao gerar {component}: {e}")
                    continue
        
        # Adiciona metadados de completude
        analysis_result['completude_documento'] = {
            'componentes_requeridos': len(required_components),
            'componentes_presentes': len([c for c in required_components if c in analysis_result]),
            'componentes_ausentes': missing_components,
            'taxa_completude': ((len(required_components) - len(missing_components)) / len(required_components)) * 100,
            'todos_componentes_incluidos': len(missing_components) == 0
        }
        
        logger.info(f"üìä Completude: {analysis_result['completude_documento']['taxa_completude']:.1f}%")
        
        return analysis_result
        
    except Exception as e:
        logger.error(f"‚ùå Erro ao garantir componentes: {e}")
        return analysis_result

def generate_missing_component(component_name: str, existing_analysis: Dict[str, Any], original_data: Dict[str, Any]) -> Optional[Dict[str, Any]]:
    """Gera componente ausente baseado no tipo"""
    
    try:
        # CORRE√á√ÉO: Gera componentes b√°sicos garantidos
        if component_name == 'projeto_dados':
            return original_data
        
        elif component_name == 'pesquisa_web_massiva':
            return ultra_detailed_analysis_engine._create_basic_research_data(original_data)
        
        elif component_name == 'avatar_ultra_detalhado':
            return ultra_detailed_analysis_engine._create_basic_avatar(original_data)
        
        elif component_name == 'insights_exclusivos':
            return ultra_detailed_analysis_engine._create_basic_insights(original_data)
        
        else:
            # Componente gen√©rico
            return {
                'nome': component_name,
                'status': 'gerado_automaticamente',
                'conteudo': f'Componente {component_name} gerado automaticamente',
                'timestamp': time.time()
            }
            
    except Exception as e:
        logger.error(f"‚ùå Erro ao gerar componente {component_name}: {e}")
        return None
@analysis_bp.route('/validate_analysis', methods=['POST'])
def validate_analysis():
    """Valida qualidade de uma an√°lise"""
    
    try:
        data = request.get_json()
        
        if not data:
            return jsonify({
                'error': 'Dados da an√°lise n√£o fornecidos'
            }), 400
        
        # Valida an√°lise
        validation_result = analysis_quality_controller.validate_complete_analysis(data)
        
        # Gera relat√≥rio
        quality_report = analysis_quality_controller.generate_quality_report(data)
        
        return jsonify({
            'validation': validation_result,
            'quality_report': quality_report,
            'can_generate_pdf': analysis_quality_controller.should_generate_pdf(data),
            'timestamp': datetime.now().isoformat()
        })
        
    except Exception as e:
        logger.error(f"Erro na valida√ß√£o: {str(e)}")
        return jsonify({
            'error': 'Erro na valida√ß√£o da an√°lise',
            'message': str(e)
        }), 500

@analysis_bp.route('/analyze_simple', methods=['POST'])
def analyze_simple():
    """Endpoint alternativo mais simples para an√°lise"""
    
    try:
        data = request.get_json()
        
        # Validar dados obrigat√≥rios
        required_fields = ['segmento', 'produto', 'publico']
        missing_fields = [field for field in required_fields if not data.get(field)]
        
        if missing_fields:
            return jsonify({
                'success': False,
                'error': f'Campos obrigat√≥rios: {", ".join(missing_fields)}'
            }), 400
        
        logger.info(f"üöÄ Iniciando an√°lise: {data.get('segmento')} - {data.get('produto')}")
        
        # Usar o engine de an√°lise existente
        from services.enhanced_analysis_engine import enhanced_analysis_engine
        
        # Gerar an√°lise completa
        analysis_result = enhanced_analysis_engine.generate_complete_analysis(data)
        
        if not analysis_result:
            return jsonify({
                'success': False,
                'error': 'Falha na gera√ß√£o da an√°lise'
            }), 500
        
        return jsonify({
            'success': True,
            'analysis': analysis_result,
            'timestamp': datetime.now().isoformat()
        })
        
    except Exception as e:
        logger.error(f"Erro na an√°lise: {str(e)}")
        return jsonify({
            'success': False,
            'error': f'Erro interno: {str(e)}'
        }), 500

@analysis_bp.route('/test_ai', methods=['POST'])
def test_ai():
    """Testa sistema de IA"""
    
    try:
        data = request.get_json()
        prompt = data.get('prompt', 'Gere um breve resumo sobre o mercado digital brasileiro em 2024.')
        
        logger.info("üß™ Testando sistema de IA...")
        
        # Testa IA
        response = ai_manager.generate_analysis(prompt, max_tokens=500)
        
        return jsonify({
            'success': bool(response),
            'prompt': prompt,
            'response': response,
            'response_length': len(response) if response else 0,
            'provider_status': ai_manager.get_provider_status(),
            'timestamp': datetime.now().isoformat()
        })
        
    except Exception as e:
        logger.error(f"Erro no teste de IA: {str(e)}")
        return jsonify({
            'error': 'Erro no teste de IA',
            'message': str(e)
        }), 500

@analysis_bp.route('/upload_attachment', methods=['POST'])
def upload_attachment():
    """Upload e processamento de anexos"""
    
    try:
        if 'file' not in request.files:
            return jsonify({
                'success': False,
                'error': 'Nenhum arquivo enviado'
            }), 400
        
        file = request.files['file']
        session_id = request.form.get('session_id', f"session_{int(time.time())}")
        
        if file.filename == '':
            return jsonify({
                'success': False,
                'error': 'Nome de arquivo vazio'
            }), 400
        
        # Processa anexo
        result = attachment_service.process_attachment(file, session_id)
        
        return jsonify(result)
        
    except Exception as e:
        logger.error(f"Erro no upload de anexo: {str(e)}")
        return jsonify({
            'success': False,
            'error': 'Erro interno no processamento do anexo',
            'message': str(e)
        }), 500

@analysis_bp.route('/list_analyses', methods=['GET'])
def list_analyses():
    """Lista an√°lises salvas"""
    
    try:
        limit = min(int(request.args.get('limit', 20)), 100)
        offset = int(request.args.get('offset', 0))
        
        analyses = db_manager.list_analyses(limit, offset)
        
        return jsonify({
            'success': True,
            'analyses': analyses,
            'count': len(analyses),
            'limit': limit,
            'offset': offset,
            'timestamp': datetime.now().isoformat()
        })
        
    except Exception as e:
        logger.error(f"Erro ao listar an√°lises: {str(e)}")
        return jsonify({
            'error': 'Erro ao listar an√°lises',
            'message': str(e)
        }), 500

@analysis_bp.route('/get_analysis/<int:analysis_id>', methods=['GET'])
def get_analysis(analysis_id):
    """Obt√©m an√°lise espec√≠fica"""
    
    try:
        analysis = db_manager.get_analysis(analysis_id)
        
        if analysis:
            return jsonify({
                'success': True,
                'analysis': analysis,
                'timestamp': datetime.now().isoformat()
            })
        else:
            return jsonify({
                'success': False,
                'error': 'An√°lise n√£o encontrada'
            }), 404
            
    except Exception as e:
        logger.error(f"Erro ao obter an√°lise {analysis_id}: {str(e)}")
        return jsonify({
            'error': 'Erro ao obter an√°lise',
            'message': str(e)
        }), 500

@analysis_bp.route('/stats', methods=['GET'])
def get_stats():
    """Obt√©m estat√≠sticas do sistema"""
    
    try:
        db_stats = db_manager.get_stats()
        ai_status = ai_manager.get_provider_status()
        search_status = production_search_manager.get_provider_status()
        
        return jsonify({
            'database_stats': db_stats,
            'ai_providers': ai_status,
            'search_providers': search_status,
            'system_health': {
                'ai_available': len([p for p in ai_status.values() if p['available']]),
                'search_available': len([p for p in search_status.values() if p['available']]),
                'database_connected': db_manager.test_connection()
            },
            'timestamp': datetime.now().isoformat()
        })
        
    except Exception as e:
        logger.error(f"Erro ao obter estat√≠sticas: {str(e)}")
        return jsonify({
            'error': 'Erro ao obter estat√≠sticas',
            'message': str(e)
        }), 500
